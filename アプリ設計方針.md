# **VRMキャラクターチャットシステム構築：PythonバックエンドとStyle-Bert-VITS2音声合成の統合**

## **I. 序論とシステム概要**

### **A. プロジェクトの要求とビジョン**

本報告書は、VRM（Virtual Reality Model）キャラクターを読み込み、ユーザーがそのキャラクターとインタラクティブにチャットできるシステムを構築するための技術的検討を行うものである。特に、バックエンド技術としてPythonを使用し、音声合成にはStyle-Bert-VITS2を利用するというユーザーの明確な要求に基づいている。インタラクティブAIキャラクターへの関心が高まる中、本システムはバーチャルアシスタント、エンターテイメント、教育ツールなど、多岐にわたる応用可能性を秘めている。

### **B. 高度システムアーキテクチャ**

提案するシステムのアーキテクチャは、クライアントサーバーモデルを採用する。主要コンポーネントは以下の通りである。

* **クライアントサイド（フロントエンド）**: ウェブブラウザ上で動作し、ユーザーインターフェース、VRMレンダリング、音声入力・出力、リップシンク（口パク同期）ロジックを担当する。  
* **サーバーサイド（バックエンド）**: PythonベースのAPIサーバーであり、チャットロジック（LLMとの通信）および音声合成（Style-Bert-VITS2）を処理する。  
* **外部サービス**: 大規模言語モデル（LLM）プロバイダー、その他必要に応じてクラウドサービス。

この構成により、表示処理とビジネスロジックを分離し、各コンポーネントの独立性と保守性を高める。AIメッセージングアーキテクチャの一般的な概念として、自然言語処理（NLP）エンジンや対話管理エージェントの重要性が指摘されているが 1、これらは本システムのバックエンドにおけるLLM連携部分で具体化される。

### **C. コア機能概要**

本システムが提供する主要機能は以下の通りである。

* VRMファイルの読み込みと3Dキャラクター表示  
* テキストおよび音声によるチャット機能  
* LLMを活用した応答生成  
* Style-Bert-VITS2による高品質な音声合成  
* 音声と同期したキャラクターのリップシンクアニメーション

### **D. 主要技術制約**

本プロジェクトにおける主要な技術的制約として、ユーザー指定により以下が定められている。

* バックエンド開発言語: Python  
* テキスト音声合成（TTS）エンジン: Style-Bert-VITS2

これらの制約を前提として、最適な技術選定とシステム設計を行う。

## **II. フロントエンドアーキテクチャと技術選択**

フロントエンドは、ユーザーとの直接的なインタラクション、VRMキャラクターの視覚的表現、そしてリアルタイム性の高い音声コミュニケーションを担当する重要なコンポーネントである。

### **A. VRMキャラクターレンダリングエンジン**

VRMキャラクターをウェブブラウザ上でリアルタイムに描画するためには、WebGLベースの3Dライブラリの選定が最初の重要なステップとなる。

#### **1\. WebGLライブラリの選択: Three.js 対 Babylon.js**

VRMレンダリングにおける主要なWebGLライブラリとして、Three.jsとBabylon.jsが比較検討される。

* **Three.js**:  
  * **強み**: 軽量であり、非常に大きなコミュニティと豊富なサンプルコードが存在する。特にVRMに関しては、@pixiv/three-vrm 2 というデファクトスタンダード的なライブラリがあり、広く利用されている。実際に、関連するオープンソースプロジェクトであるChatVRMもThree.jsと@pixiv/three-vrmを採用しており 4、これは強力な採用根拠となる。@pixiv/three-vrmはVRMの読み込み、MToonマテリアルのサポート、そして開発が進むWebGPUへの対応といった特徴を持つ 2。  
  * **考慮点**: React Three Fiberのような高レベルな抽象化レイヤーを使用しない場合、一部の側面で学習曲線が急になる可能性がある 7。  
* **Babylon.js**:  
  * **強み**: より「オールインワン」なゲームエンジンに近い使用感があり、公式ドキュメントも充実している。公式のVRMローダー (babylon-vrm-loader) が提供されており 9、ゲームエンジンに慣れた開発者にとっては扱いやすい可能性がある 7。babylon-vrm-loaderはVRM 0.x、ブレンドシェイプ、SpringBoneをサポートしている 10。  
  * **考慮点**: Three.jsと比較してファイルサイズが大きく、VRMに関するコミュニティやエコシステムはThree.jsほど大きくない可能性がある。

**推奨**: ウェブアプリケーションにおけるVRMの広範な採用実績、ChatVRMのようなプロジェクトによって実証された強力なコミュニティサポート、そして@pixiv/three-vrmの重点的な開発状況を考慮し、**Three.jsと@pixiv/three-vrmの組み合わせを推奨**する。Three.jsの選択は、@pixiv/three-vrm 2 の利用可能性と活発な開発、そしてChatVRM 4 のような著名なオープンソースプロジェクトでの採用実績によって裏付けられる。このエコシステムの相乗効果は、この特定のユースケースにおいて代替案と比較してより成熟し、テストされたウェブベースのVRMアプリケーション開発経路を示唆しており、開発リスクを低減し、豊富なコミュニティ知識へのアクセスを提供する。

**表1: VRMレンダリングのためのWebGLライブラリ比較 (Three.js vs. Babylon.js)**

| 特徴/基準 | Three.js (@pixiv/three-vrm) | Babylon.js (babylon-vrm-loader) |
| :---- | :---- | :---- |
| VRMローダーの利用可能性と成熟度 | 高い (@pixiv/three-vrmが活発に開発) 2 | 良好 (babylon-vrm-loaderが公式提供) 10 |
| パフォーマンスに関する考慮事項 | 軽量、WebGPU対応進行中 2 | 最適化されているが、Three.jsよりファイルサイズ大 7 |
| VRMに関するコミュニティとエコシステム | 非常に活発、ChatVRM等実績多数 4 | 成長中だがThree.jsに比べ小規模 |
| VRMタスクの学習曲線 | R3F等未使用時はやや急 7 | 比較的緩やか、エンジンライク 7 |
| JSフレームワークとの連携 | React Three Fiber等で強力に連携 3 | Vue-babylonjs等で連携可能 13 |
| 主要VRM機能サポート | MToon, SpringBone, VRM1.0 (ローダー依存) 2 | MToon, SpringBone, VRM0.x (ローダー依存) 10 |

#### **2\. VRMの読み込みとシーン初期化**

選択したライブラリを用いてVRMファイルを読み込み、シーンを初期化する手順は以下のようになる。Three.jsと@pixiv/three-vrmを使用する場合、GLTFLoaderにVRMLoaderPluginを登録して.vrmファイルをロードする 2。

1. Three.js（またはBabylon.js）のシーン、カメラ、光源を設定する。  
2. GLTFLoader（Three.jsの場合）または対応するローダー（Babylon.jsの場合）を使用してVRMファイルを非同期に読み込む 3。  
3. 読み込まれたVRMモデルをシーンに追加し、位置やスケールを調整する。

#### **3\. VRM標準の理解: ボーンとブレンドシェイプ**

VRMファイル形式は、ポージングのための人型ボーン構造と、リップシンクを含む表情アニメーションのためのブレンドシェイプ（モーフィングターゲット）を標準で定義している 14。特にリップシンクにおいては、「あ(A)」「い(I)」「う(U)」「え(E)」「お(O)」といった標準ブレンドシェイプの存在が重要となる 14。これらの標準化された要素を理解し活用することで、異なるVRMモデル間でもある程度の互換性を保ちつつ、豊かな表現が可能になる。

### **B. ユーザーインターフェース（UI）フレームワーク**

インタラクティブなウェブアプリケーションを構築するには、効率的なUIフレームワークの選択が不可欠である。

#### **1\. フロントエンドフレームワークの評価: React, Vue, Svelte**

* **React**: 巨大なエコシステムと豊富な人材プールを持ち、複雑なアプリケーションに適している。特にThree.jsとの連携においては、React Three Fiber (R3F) が優れた開発体験を提供する 3。  
* **Vue.js**: 一部開発者にとっては学習曲線が緩やかで、良好なパフォーマンスを持つ。Three.jsとの連携はTresJS（R3Fに触発され進化中）19、Babylon.jsとはvue-babylonjs 13 などのライブラリを介して可能。  
* **Svelte**: コンパイラベースであり、多くの場合、最高のネイティブパフォーマンスと最小のバンドルサイズを実現する。Three.jsとの連携にはThrelteがある 22。エコシステムと人材プールはReactやVueに比べて小さい 17。

#### **2\. 推奨と根拠**

レンダリングエンジンとしてThree.jsを選択したことを踏まえ、**ReactとReact Three Fiber (R3F) の組み合わせを強く推奨**する。R3Fは成熟度が高く、ドキュメントも豊富で、大規模なコミュニティが存在するため、複雑なThree.jsのシーン管理を宣言的なReactの構造内で簡潔に行うことができる。この選択は、最新のウェブ開発プラクティスとよく整合し、特にReactに慣れたチームにとっては開発者の生産性とコードの保守性を大幅に向上させることができる。ChatVRMプロジェクト 4 がNext.js（Reactフレームワーク）と@pixiv/three-vrmで構築されていることも、この方向性を支持する。

ただし、開発チームがVueやSvelteに強い専門知識を持つ場合は、それぞれのThree.js連携ライブラリ（TresJS、Threlte）も実行可能な代替案となる。

#### **3\. UIコンポーネント設計**

主要なUIコンポーネントとして以下が想定される。

* チャット入力フィールド（テキスト用）  
* マイク切り替えボタン  
* VRMモデル表示エリア  
* 設定パネル（例: VRM読み込み、音声選択など）

### **C. 音声入力: Web Speech API**

ユーザーの音声をブラウザで直接取得するためには、Web Speech APIのSpeechRecognitionインターフェースを利用する 23。onresultイベントで文字起こしされたテキストを取得し、onerrorイベントでエラーを処理する。ブラウザの互換性、マイク使用許可の処理、連続認識と単発認識の選択などが考慮事項となる。

### **D. TTS用音声再生と管理**

バックエンドから受信した合成音声を再生するには、HTML5の\<audio\>要素またはWeb Audio APIを使用する。特に、チャンク化/ストリーミングされたTTSを実現する場合は、オーディオストリームの管理が重要になる。音声再生とリップシンクアニメーションの同期については、セクションVで詳述する。この音声データは、FastAPIのレスポンスタイプ（例: StreamingResponse）を介して提供されることが想定され、クライアント側での適切な処理が求められる 25。

Style-Bert-VITS2 27 やElevenLabs 29 (ChatVRMで使用 5) のような一般的なTTS APIは、正確なリップシンクに必要な詳細なビゼーム（音素の視覚的表現）やタイミングデータをネイティブには出力しない。このため、このデータを生成するか、音声分析を行う負担がクライアント側に生じる可能性が高い。これは、音声処理とアニメーションのためのフロントエンドライブラリの選択を非常に重要なものにする（セクションV参照）。

## **III. バックエンドアーキテクチャと技術選択（Python中心）**

バックエンドは、Pythonという制約の下、チャットロジック、LLM連携、そしてStyle-Bert-VITS2による音声合成というコア機能を実現する。

### **A. コアAPIフレームワーク: FastAPI**

#### **1\. FastAPIを選択する理由**

* **高性能**: ASGIベースであり、StarletteとPydanticを活用することで高いパフォーマンスを実現する 31。  
* **ネイティブ非同期サポート**: LLM呼び出しやTTS生成のようなI/Oバウンドな操作に不可欠な非同期処理をネイティブでサポートする。  
* **自動データバリデーションとシリアライゼーション**: Pydanticによる型ヒントベースの自動的なデータ検証とシリアライズ機能を提供する。  
* **WebSocketの組み込みサポート**: リアルタイム双方向通信のためのWebSocketを標準でサポートしている 33。  
* **ML/AI API構築への適性**: 機械学習やAIモデルを組み込んだAPIの構築に適している 31。

**表2: PythonバックエンドAPIフレームワーク比較 (FastAPI vs. Flask)**

| 特徴/基準 | FastAPI | Flask |
| :---- | :---- | :---- |
| パフォーマンス (req/s) | 高い (ASGI) 31 | FastAPIより低い (WSGI) 31 |
| 非同期サポート | ネイティブ 31 | 限定的 (別途ライブラリ要) |
| データバリデーション | Pydanticによる自動検証 31 | 手動または拡張機能 31 |
| WebSocketサポート | 標準装備 33 | 拡張機能 (Flask-Sockets等) |
| ML連携の容易さ | 高い 34 | 良好 |
| 学習曲線 | 比較的緩やか 32 | 非常に緩やか 32 |
| コミュニティサイズ | 急成長中、活発 32 | 大規模、成熟 32 |

#### **2\. プロジェクト構造とセットアップ**

標準的なFastAPIアプリケーションのディレクトリ構造を採用し、依存関係管理にはPoetryまたはpipとrequirements.txtファイルを使用する。

### **B. チャットロジックとLLM連携**

#### **1\. LLM連携戦略の選択**

* **LangChain**: 様々なLLMに対する抽象化レイヤーを提供し、プロンプト管理、チェイン、エージェント、メモリ機能などを簡素化する 35。複雑なLLMワークフローの構築を容易にし、FastAPIとの連携例も存在する 37。  
* **Hugging Face Transformers/API直接利用**: transformersライブラリ 39 を用いて自己ホスト型モデルを利用するか、Hugging Face Inference Endpoints 34 を呼び出す。より詳細な制御が可能。

**推奨**: **LangChain**から始めることを推奨する。その理由は、多様なLLMの柔軟な統合、会話履歴/メモリ管理の容易さにある。LangChainでは提供されない特定のモデル制御が必要になった場合に、直接APIを利用するアプローチを検討できる。LLMの状況は急速に進化しており、LangChain 35 を使用することで、アプリケーションロジックを特定のLLM APIから分離できる。これにより、コード変更を最小限に抑えながら、さまざまなモデルでの実験やプロバイダーの切り替えが容易になり、将来性と柔軟性が提供される。また、メモリ管理や複雑なインタラクションパターン（エージェント、ツール）も標準化される。

#### **2\. チャットAPIエンドポイントの設計**

ユーザーメッセージの受信とLLM応答のストリーミングのために、WebSocketエンドポイントが適している。

* 入力: ユーザーのテキストメッセージ  
* 処理: テキストをLLMに渡す（LangChain経由）  
* 出力: LLMのテキスト応答をクライアントにストリーミング

#### **3\. 会話コンテキスト/メモリ管理**

LangChainは、一貫性のある対話に不可欠な会話履歴を維持するためのメモリモジュールを提供する 35。あるいは、バックエンドで単純なセッションベースの履歴管理も可能である。

### **C. Style-Bert-VITS2による音声合成**

#### **1\. Style-Bert-VITS2のサービスとしてのセットアップ**

Style-Bert-VITS2は独自のAPIサーバー（server\_fastapi.py）を実行できるが 27、より緊密な統合のためには、Style-Bert-VITS2の推論ロジックを直接FastAPIエンドポイント内にラップすることを推奨する。このアプローチの優れた例として、FastAPIを使用したStyle-Bert-VITS2用の軽量APIサーバーが存在する 41。これにより、個別のサービスを管理する手間が省ける。インストールにはPython、PyTorch、その他の依存関係が必要であり 27、推論にはモデルファイル（config.json、.safetensors、style\_vectors.npy）が不可欠である 27。Style-Bert-VITS2は独自のサーバーを提供しているが 27、LLMもさまざまな手段でアクセスできるため、すべてのバックエンドロジック（チャット、TTS、WebSocket）を単一のFastAPIアプリケーションに統合することで（SB-VITS2については41、LangChainについては37で実証）、開発、デプロイ、およびサービス間通信が簡素化される。FastAPIの非同期機能は、同時LLMおよびTTSリクエストを効率的に処理するのに非常に適している。

#### **2\. TTS用Python API**

テキストを受け取り、合成された音声を返すFastAPIエンドポイントを設計する。

* 入力: 合成するテキスト、話者ID（該当する場合）、スタイルパラメータ  
* 処理: Style-Bert-VITS2の推論関数を呼び出す。  
* 出力: 音声データ（例: WAV形式）。FastAPIのResponseまたはStreamingResponseを使用して音声を返すことができる 25。litestsライブラリ 43 は、SpeechGateway経由でStyle-Bert-VITS2を含むさまざまなTTSの抽象化も示しており、後でより多くのTTSオプションが必要になった場合の代替統合パスとなる可能性がある。

#### **3\. 音声フォーマットと配信**

Style-Bert-VITS2は通常WAV形式で音声を出力する。フロントエンドがこれを処理できることを確認する必要がある。長い応答に対しては、知覚されるレイテンシを削減するために音声ストリーミングを検討する（例: FastAPIのStreamingResponse 25）。

LLM推論とTTS生成はどちらも時間がかかる可能性がある。FastAPIのasync/await構文 31 は非常に重要である。await websocket.receive\_text() 33 のような操作、LLMの呼び出し、そしてStyle-Bert-VITS2の呼び出し（41ではawait sbv.tts(...)としてawait可能になっている）はすべて非同期であるべきであり、ブロッキングを防ぎ、サーバーが複数の同時ユーザーを効率的に処理できるようにする。これは、応答性の高いAIアプリケーションの基本的なアーキテクチャ原則である。

## **IV. フロントエンド・バックエンド間通信**

フロントエンドとバックエンド間のスムーズなデータ交換は、リアルタイムインタラクションの鍵となる。

### **A. チャットインタラクション: WebSockets**

#### **1\. WebSockets採用の根拠**

* 全二重の永続的な通信チャネルを確立し、リアルタイムチャットに最適である 44。  
* メッセージ交換のための繰り返しのHTTPリクエストと比較してレイテンシが低い。  
* サーバーがLLMの応答やTTS音声（または音声準備完了通知）をクライアントに積極的にプッシュできる。

#### **2\. FastAPIによるWebSocketエンドポイントの実装**

FastAPIでは、@app.websocket("/ws")デコレータを使用してWebSocketエンドポイントを簡単に実装できる 33。接続の受け入れ、テキスト/バイナリデータの送受信、接続のクローズといった接続管理を行う。FastAPIのWebSocketを使用した簡単なチャットサーバーの例が参考になる 33。

#### **3\. WebSocket経由のデータフロー**

1. クライアントは、文字起こしされた音声（テキストとして）または入力されたテキストをバックエンドに送信する。  
2. バックエンドは、LLMのテキスト応答をクライアントに送信する。  
3. バックエンドは、合成された音声データ（またはそのURL）をクライアントに送信する。これはWebSocket経由でバイナリデータとして送信することも、HTTP経由で取得するための通知として送信することも可能である。

### **B. TTS音声取得用API (HTTP/REST)**

音声がWebSocket経由で直接ストリーミングされない場合、生成された音声ファイルを提供するためのHTTPエンドポイントをFastAPIで用意する。

1. クライアントは（例えばWebSocket経由で）音声の準備ができたという通知とURLを受信する。  
2. クライアントはこのURLに対してHTTP GETリクエストを行い、音声を取得する。

FastAPIのFileResponseまたはStreamingResponseを使用して音声ファイルを提供できる 25。これにより、ブラウザは\<audio\>タグを使用して自然に音声のキャッシュと再生を処理できる。

リアルタイムのテキストチャットにはWebSocket 33 が優れているが、すべてのTTS応答に対して大きな音声バイナリデータをWebSocket経由で直接ストリーミングすると、クライアントとサーバーの両方で処理（バッファリング、再構築）が複雑になる可能性がある。ハイブリッドアプローチ、つまりテキストメッセージとリアルタイム通知にはWebSocketを使用し、実際の音声ファイルの取得にはHTTP REST（FastAPIのFileResponse 25 を使用）を利用することで、両者の長所を活かすことができる。HTTPはブラウザが音声を効果的にキャッシュし、標準の\<audio\>要素機能を使用することを可能にする。

また、TTS生成には時間がかかる。バックエンドはフロントエンドを無為に待たせるべきではない。合成用のテキストを受信した後、バックエンドはリクエストを確認し（おそらくWebSocket経由で）、Style-Bert-VITS2によって音声が生成されたら、音声を取得するためのURLを含む別のWebSocketメッセージをクライアントに送信する。この非同期通知パターンは、知覚される応答性を向上させる。

### **C. 初期VRM読み込みと設定 (HTTP/REST)**

利用可能なVRMモデルのリスト（選択肢が提供される場合）やその他の初期設定データを取得するためのエンドポイントは、標準的なHTTP/REST APIとして実装できる。

## **V. リップシンク実装**

キャラクターの口の動きを音声と同期させるリップシンクは、インタラクションのリアリズムを高める上で非常に重要である。

### **A. コア原則の概要: ブレンドシェイプアニメーション**

VRMモデルは、音声に合わせた口の形を含む表情のためにブレンドシェイプ（モーフィングターゲット）を使用する 14。リップシンクは、これらのブレンドシェイプのinfluence（影響度）を時間経過とともに変化させて、話されている音声と一致させることで実現される。リップシンクのための主要なVRMブレンドシェイプには、「あ(A)」「い(I)」「う(U)」「え(E)」「お(O)」などがある（無音のための「Sil」なども考えられる）。

### **B. 同期データ（ビゼームとタイミング）の生成**

Style-Bert-VITS2 27 や、ChatVRMで使用されているElevenLabs 5 のような多くのTTS APIは、高品質なリップシンクに必要なビゼーム（音素の視覚的表現）や正確なタイミングデータを直接提供しないという課題がある。この「ビゼームのギャップ」は、Rhubarb Lip SyncのようなツールをWebAssembly経由でクライアントサイドで処理することによって埋める必要がある可能性が高い 48。これはフロントエンドアーキテクチャに大きな影響を与え、堅牢な音声処理とWASM統合が必要となる。

* オプション1 (理想的だがStyle-Bert-VITS2では困難): TTS提供のビゼーム  
  TTSエンジンが音声と同時にビゼームのリストとその開始/終了時刻を提供できれば、これが最も直接的なアプローチとなる（現在の調査ではStyle-Bert-VITS2はこの機能をサポートしていない）。  
* オプション2 (クライアントサイド分析 \- 推奨): WebAssembly経由のRhubarb Lip Sync  
  Rhubarb Lip Syncは、音声と対話テキストを分析して、タイミング付きのビゼームデータを生成するツールである 48。AからXまでの口の形状セットをサポートする。Rhubarbや同様のツールのWebAssembly (WASM) 版を使用すると、この分析をブラウザ内で実行できる 49。  
  ワークフロー:  
  1. フロントエンドがバックエンドから音声（Style-Bert-VITS2製）を受信する。  
  2. フロントエンドは対応するテキストも受信する。  
  3. 音声とテキストをRhubarb WASMモジュールに渡す。  
  4. Rhubarbはビゼームのシーケンス（例: {time: 0.1, viseme: 'A'}, {time: 0.25, viseme: 'B'},...）を出力する。  
* オプション3 (より単純だが精度は低い): ボリュームベースのリップシンク  
  Web Audio APIのAnalyserNodeを使用してクライアントサイドで音声ボリュームを分析する（ChatVRMのLipSyncクラスはボリュームにこれを使用している 53）。ボリュームレベルを単純な口の開閉ブレンドシェイプにマッピングする。初歩的だが実装は容易である。

**表3: WebベースVRMのためのリップシンクアプローチ比較**

| 手法 | 利点 | 欠点 | 実装複雑度 | 必要なライブラリ/ツール | 期待品質 |
| :---- | :---- | :---- | :---- | :---- | :---- |
| TTS提供ビゼーム | 最も正確、処理負荷低い | Style-Bert-VITS2等では非対応 27 | 低 | TTSエンジン依存 | 高 |
| Rhubarb.js/WASM (クライアント) | 高品質なビゼーム生成、オフライン処理可能 | WASM統合、Rhubarbビゼームのマッピングが必要 48 | 中～高 | Rhubarb WASM, 音声処理ライブラリ | 中～高 |
| 基本的なボリューム分析 (クライアント) | 実装が容易、軽量 | 単純な口の開閉のみ、表現力低い 53 | 低 | Web Audio API (AnalyserNode) | 低 |

### **C. フロントエンドでのVRMブレンドシェイプアニメーション**

ビゼームのタイミングデータ（例: Rhubarb WASMから）が得られたら、フロントエンドのアニメーションループ（例: requestAnimationFrame またはR3FのuseFrame）で以下を実行する。

1. Rhubarbのビゼーム（A-X）をVRMモデルの利用可能なブレンドシェイプ（A, I, U, E, O, Neutralなど）にマッピングする。このマッピングには創造的な解釈が必要になる場合がある 14。このマッピングはリップシンクの自然さに影響を与える。  
2. HTML5オーディオ要素の現在の再生時間を取得する。正確なリップシンクは、ブレンドシェイプアニメーションを再生中の音声と正確に連携させることに依存する。HTML5 \<audio\> 要素のイベント（play, pause, seeking, seeked, timeupdate）とプロパティ（currentTime, duration）はこれに不可欠である 26。フロントエンドのアニメーションループは、任意の瞬間に正しいビゼームを駆動するために audioElement.currentTime を照会する必要がある。  
3. この時間とRhubarbの出力に基づいて現在のビゼームを決定する。  
4. 対応するVRMブレンドシェイプのinfluence値を滑らかに補間する。例えば、現在のビゼームが「A」であれば、vrm.expressionManager.setValue('A', 1.0)（@pixiv/three-vrmの場合、UniVRMのVRMBlendShapeProxy 16 と概念的に類似）とし、他を0.0にするか、より滑らかな遷移のために補間を使用する 54。

## **VI. システム統合とワークフロー**

各コンポーネントを連携させ、スムーズなユーザー体験を実現するための統合ワークフローを定義する。

### **A. エンドツーエンドのデータフロー（テキスト記述）**

1. **音声入力 (クライアント)**: ユーザーが発話 → Web Speech APIがテキストに文字起こし。  
2. **テキストのバックエンド送信 (クライアント → サーバー)**: 文字起こしされたテキストをWebSocket経由でFastAPIバックエンドに送信。  
3. **LLM処理 (サーバー)**: FastAPIがテキスト受信 → LangChainが処理（コンテキスト管理、プロンプトエンジニアリング）→ 外部LLM API呼び出し。  
4. **LLM応答 (サーバー)**: LLM APIがテキスト応答を返す → LangChainが後処理 → FastAPIが最終テキスト取得。  
5. **TTSリクエスト (サーバー)**: FastAPIがLLMのテキスト応答をStyle-Bert-VITS2モジュール/サービスに送信。  
6. **TTS音声生成 (サーバー)**: Style-Bert-VITS2がWAV音声を生成。  
7. **音声配信 (サーバー → クライアント)**:  
   * オプションA (単純さとキャッシュのため推奨): FastAPIが音声URLを含むWebSocketメッセージを送信 → クライアントがHTTP GETで音声を取得。  
   * オプションB: FastAPIがWebSocket経由で音声バイナリをストリーミング。  
8. **音声再生とリップシンクデータ生成 (クライアント)**: クライアントが音声受信 → HTML5 \<audio\> で音声再生 → 同時に音声（とテキスト）をRhubarb WASMに渡しビゼーム生成。  
9. **VRMアニメーション (クライアント)**: Rhubarbがビゼームタイミングを出力 → クライアントサイドアニメーションロジックがビゼームをVRMブレンドシェイプにマッピングし、音声のcurrentTimeと同期してinfluenceを更新。  
10. **キャラクター応答 (クライアント)**: ユーザーは、同期したリップシンクで話すVRMキャラクターを見て、合成された音声を聞く。

このエンドツーエンドのワークフローには、複数の連続したステップ（STT → LLM → TTS → 音声転送 → リップシンク生成 → アニメーション）が含まれる。各ステップで遅延が発生するため、各コンポーネントの最適化（高速なLLM、効率的なStyle-Bert-VITS2、迅速なRhubarb処理など）と、可能な箇所でのストリーミング（LLM応答テキスト、音声データ）の利用が、応答性の高いユーザー体験のために不可欠である。知覚される遅延は、対話型AIアプリケーションのユーザビリティにおける主要な要因となる。

### **B. 状態管理に関する考慮事項**

* **フロントエンド**: UI状態（録音中か、発話中か、チャット履歴など）、VRMモデルの状態、音声再生状態を管理する。Reactを使用する場合はZustandやRedux Toolkit、Vueを使用する場合はVuexやPiniaのようなライブラリが役立つ。  
* **バックエンド**: LLMの会話履歴/コンテキストを管理する（LangChainのメモリモジュールで対応可能 35）。HTTP GETで提供する場合、生成された音声ファイルの一時的なストレージを管理する。

### **C. エラー処理と回復性**

API障害（LLM、TTS）、WebSocket切断、マイク許可の問題、VRM読み込み失敗などに対する丁寧なエラー処理を実装する。長時間の操作中（例: 「キャラクターは考えています...」、「音声を生成しています...」）にユーザーにフィードバックを提供する。多くの操作は非同期（API呼び出し、音声デコード、WASM処理）であるため、フロントエンドUIは常にレスポンシブである必要がある。これは、JavaScriptでのasync/awaitの広範な使用、Rhubarb WASMが遅い場合のWeb Workerへの重い計算のオフロード、そしてアプリケーションがフリーズしているように見えないようにユーザーへの視覚的なフィードバックの提供を意味する。

## **VII. 主要な考慮事項と将来の機能強化**

システムの基本的な枠組みを超えて、品質、拡張性、および将来性を見据えた考慮事項を以下に示す。

### **A. パフォーマンス最適化**

* **フロントエンドレンダリング**: VRMモデルの複雑さ（ポリゴン数、テクスチャ）、Three.js/Babylon.jsシーンの最適化、描画呼び出しの削減。@pixiv/three-vrmにはVRMUtils.removeUnnecessaryJointsなどの最適化関数がある 12。  
* **バックエンドAPI**: 効率的なLLMプロンプティング、該当する場合の一般的なフレーズに対するTTS結果のキャッシュ（41のStyle-Bert-VITS2 APIには単純なキャッシュが含まれている）。  
* **音声処理**: 適切な音声フォーマットとストリーミング戦略の選択。

### **B. スケーラビリティ**

* 同時ユーザーに対応するためのバックエンドサーバー容量（FastAPIとUvicorn/Gunicornワーカー 56）。  
* 外部API呼び出し（LLM、TTS）のレート制限。

### **C. セキュリティ**

* LLMおよびTTSサービスのAPIキーの保護（バックエンドに保存し、クライアントには決して公開しない）。  
* すべてのAPIエンドポイントの入力検証。  
* フロントエンドとバックエンドが異なるドメインにある場合のCORS設定。

### **D. キャラクターカスタマイズと感情表現**

* ユーザーが独自のVRMファイルをアップロードできるようにする。  
* LLMの感情分析に基づいてリップシンクを拡張し、感情表現を含める（例: 感情をVRMの「喜び」「怒り」などのブレンドシェイプにマッピングする 14）。ChatdollKitはこの機能に言及している 57。基本的なリップシンクは機能的な発話を提供するが、LLMの応答のトーンと同期した表情を通じて感情を伝えることで 14、ユーザーエンゲージメントとキャラクターの知性/信憑性を大幅に向上させることができる。これには、LLMの感情分析と、それをVRMの感情ブレンドシェイプにマッピングする処理が含まれる。

### **E. 高度な音声機能**

* Style-Bert-VITS2で異なる音声スタイルや話者を試す 27。  
* Style-Bert-VITS2をカスタムデータでトレーニングした場合のボイスクローニングの可能性。Style-Bert-VITS2は多様な音声スタイルを可能にするが 27、特定のキャラクターボイスを作成するには、多くの場合、カスタム音声データでモデルをトレーニングする必要がある。このプロセス（データセット作成、トレーニング 27）は複雑で時間がかかり、事前トレーニングされたオプションを超えた高度にパーソナライズされたキャラクターボイスを望むユーザーにとっては大きなハードルとなる可能性がある。深いキャラクターのパーソナライズが目標である場合、これは将来の焦点領域となる可能性がある。

### **F. クロスブラウザ互換性**

対象ブラウザ全体でWebGL、Web Speech API、Web Audio API、およびWebSocketのサポートをテストする。

## **VIII. 結論と推奨事項**

本報告書では、VRMキャラクターをロードし、PythonバックエンドとStyle-Bert-VITS2音声合成を使用してチャットできるシステムの構築について、詳細なアーキテクチャと技術的検討を行った。

提案されたアーキテクチャは、フロントエンドにThree.js（@pixiv/three-vrmを利用）とReact（React Three Fiberを推奨）、バックエンドにFastAPIを採用し、リアルタイム通信には主にWebSocketを使用する。音声入力にはWeb Speech API、音声合成には指定通りStyle-Bert-VITS2をFastAPI経由で利用し、リップシンクはクライアントサイドでRhubarb Lip SyncのWebAssembly版を用いて実現することを推奨する。

**主要な推奨事項:**

1. **技術スタックの確定**:  
   * **フロントエンド**: Three.js \+ @pixiv/three-vrm \+ React (with React Three Fiber)  
   * **バックエンド**: FastAPI \+ LangChain \+ Style-Bert-VITS2 (FastAPIラッパー)  
   * **リップシンク**: Rhubarb Lip Sync (WebAssembly)  
2. **段階的開発**: まずコアとなるVRM表示、テキストチャット、基本的な音声合成（リップシンクなし）を実装し、その後、音声入力、リップシンク、感情表現といった高度な機能を段階的に追加していくアプローチが現実的である。  
3. **リップシンクの品質向上への注力**: Style-Bert-VITS2から直接ビゼーム情報が得られないため、Rhubarb WASMの出力とVRMブレンドシェイプのマッピング調整、および音声とアニメーションの同期精度向上が、自然なインタラクション実現の鍵となる。  
4. **パフォーマンスとUXの継続的最適化**: 特にLLMの応答時間、TTSの生成時間、音声データの転送時間といったクリティカルパスにおける遅延を最小化し、ユーザーに適切なフィードバックを提供することで、体感的な応答性を高める必要がある。  
5. **コミュニティリソースの活用**: Three.js、@pixiv/three-vrm、FastAPI、LangChain、Style-Bert-VITS2はいずれも活発なコミュニティや豊富なドキュメントが存在するため、これらを積極的に活用することが開発効率向上に繋がる。特にChatVRM 4 やChatdollKit 57 のような関連プロジェクトは、実装の参考となるだろう。

本報告書で提示されたアーキテクチャと技術選定は、ユーザーの要求を満たしつつ、拡張性と保守性を備えたVRMキャラクターチャットシステムを構築するための強固な基盤を提供するものである。

#### **引用文献**

1. The Build: AI Messaging Architecture Decoded \- Vizologi, 5月 17, 2025にアクセス、 [https://vizologi.com/build-ai-messaging-architecture-decoded/](https://vizologi.com/build-ai-messaging-architecture-decoded/)  
2. pixiv/three-vrm: Use VRM on Three.js \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/pixiv/three-vrm](https://github.com/pixiv/three-vrm)  
3. How to Create a VTuber Studio with Three.js, React & VRM \- Wawa Sensei, 5月 17, 2025にアクセス、 [https://wawasensei.dev/tuto/vrm-avatar-with-threejs-react-three-fiber-and-mediapipe](https://wawasensei.dev/tuto/vrm-avatar-with-threejs-react-three-fiber-and-mediapipe)  
4. camenduru/ChatVRM-LocalAI: Chat with VRM avatar via LLM \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/camenduru/ChatVRM-LocalAI](https://github.com/camenduru/ChatVRM-LocalAI)  
5. zoan37/ChatVRM: Chat with VRM avatar via LLM \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/zoan37/ChatVRM](https://github.com/zoan37/ChatVRM)  
6. pixiv/three-vrm-materials-mtoon \- GitHub Pages, 5月 17, 2025にアクセス、 [https://pixiv.github.io/three-vrm/packages/three-vrm-materials-mtoon/docs/](https://pixiv.github.io/three-vrm/packages/three-vrm-materials-mtoon/docs/)  
7. Babylon.js Vs. Three.js-comparison, 5月 17, 2025にアクセス、 [https://ansibytecode.com/babylon-js-vs-three-js-comparison/](https://ansibytecode.com/babylon-js-vs-three-js-comparison/)  
8. Three.js vs. Babylon.js: Which is better for 3D web development? \- LogRocket Blog, 5月 17, 2025にアクセス、 [https://blog.logrocket.com/three-js-vs-babylon-js/](https://blog.logrocket.com/three-js-vs-babylon-js/)  
9. Babylon.js and VRM, 5月 17, 2025にアクセス、 [https://doc.babylonjs.com/communityExtensions/Babylon.js+ExternalLibraries/BabylonJS\_and\_VRM/](https://doc.babylonjs.com/communityExtensions/Babylon.js+ExternalLibraries/BabylonJS_and_VRM/)  
10. virtual-cast/babylon-vrm-loader: glTF VRM extension ... \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/virtual-cast/babylon-vrm-loader](https://github.com/virtual-cast/babylon-vrm-loader)  
11. babylon-vrm-loader/.hintrc at master \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/virtual-cast/babylon-vrm-loader/blob/master/.hintrc](https://github.com/virtual-cast/babylon-vrm-loader/blob/master/.hintrc)  
12. VRMUtils | @pixiv/three-vrm, 5月 17, 2025にアクセス、 [https://pixiv.github.io/three-vrm/docs/classes/three-vrm.VRMUtils.html](https://pixiv.github.io/three-vrm/docs/classes/three-vrm.VRMUtils.html)  
13. Beg-in/vue-babylonjs: A ready-to-go 3d environment for Vue.js using Babylon.js \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/Beg-in/vue-babylonjs](https://github.com/Beg-in/vue-babylonjs)  
14. Features and contents of VRM, 5月 17, 2025にアクセス、 [https://vrm.dev/en/vrm/vrm\_features/](https://vrm.dev/en/vrm/vrm_features/)  
15. vrm-specification/specification/VRMC\_vrm-1.0/README.md at master · vrm-c/vrm ... \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/vrm-c/vrm-specification/blob/master/specification/VRMC\_vrm-1.0/README.md](https://github.com/vrm-c/vrm-specification/blob/master/specification/VRMC_vrm-1.0/README.md)  
16. BlendShape Setting \- VRM, 5月 17, 2025にアクセス、 [https://vrm.dev/en/univrm/blendshape/univrm\_blendshape/](https://vrm.dev/en/univrm/blendshape/univrm_blendshape/)  
17. Comparing front-end frameworks for startups in 2025: Svelte vs React vs Vue \- Merge Rocks, 5月 17, 2025にアクセス、 [https://merge.rocks/blog/comparing-front-end-frameworks-for-startups-in-2025-svelte-vs-react-vs-vue](https://merge.rocks/blog/comparing-front-end-frameworks-for-startups-in-2025-svelte-vs-react-vs-vue)  
18. JavaScript Frameworks in 2024: React vs. Vue vs. Svelte – Which One to Choose?, 5月 17, 2025にアクセス、 [https://dev.to/tarunsinghofficial/javascript-frameworks-in-2024-react-vs-vue-vs-svelte-which-one-to-choose-4c0p](https://dev.to/tarunsinghofficial/javascript-frameworks-in-2024-react-vs-vue-vs-svelte-which-one-to-choose-4c0p)  
19. Integration with Vue 3 : r/threejs \- Reddit, 5月 17, 2025にアクセス、 [https://www.reddit.com/r/threejs/comments/1i6vdzr/integration\_with\_vue\_3/](https://www.reddit.com/r/threejs/comments/1i6vdzr/integration_with_vue_3/)  
20. ThreeJS projects with complex business logic, but write by vanilla JS? \- Questions, 5月 17, 2025にアクセス、 [https://discourse.threejs.org/t/threejs-projects-with-complex-business-logic-but-write-by-vanilla-js/48012](https://discourse.threejs.org/t/threejs-projects-with-complex-business-logic-but-write-by-vanilla-js/48012)  
21. How to use BabylonJS with Vue | Babylon.js Documentation, 5月 17, 2025にアクセス、 [https://doc.babylonjs.com/communityExtensions/Babylon.js+ExternalLibraries/BabylonJS\_and\_Vue/BabylonJS\_and\_Vue\_1](https://doc.babylonjs.com/communityExtensions/Babylon.js+ExternalLibraries/BabylonJS_and_Vue/BabylonJS_and_Vue_1)  
22. Can't use Three.js in my SvelteKit website : r/sveltejs \- Reddit, 5月 17, 2025にアクセス、 [https://www.reddit.com/r/sveltejs/comments/1ag6wek/cant\_use\_threejs\_in\_my\_sveltekit\_website/](https://www.reddit.com/r/sveltejs/comments/1ag6wek/cant_use_threejs_in_my_sveltekit_website/)  
23. Web Speech API: A Beginner's Guide \- F22 Labs, 5月 17, 2025にアクセス、 [https://www.f22labs.com/blogs/web-speech-api-a-beginners-guide/](https://www.f22labs.com/blogs/web-speech-api-a-beginners-guide/)  
24. Speech recognition in the browser using Web Speech API \- AssemblyAI, 5月 17, 2025にアクセス、 [https://www.assemblyai.com/blog/speech-recognition-javascript-web-speech-api](https://www.assemblyai.com/blog/speech-recognition-javascript-web-speech-api)  
25. Custom Response \- HTML, Stream, File, others \- FastAPI, 5月 17, 2025にアクセス、 [https://fastapi.tiangolo.com/advanced/custom-response/](https://fastapi.tiangolo.com/advanced/custom-response/)  
26. Synchronize HTML5 Audio With Animations \- Hans Garon, 5月 17, 2025にアクセス、 [https://hansgaron.com/articles/web\_audio/audio\_tag\_and\_scrubbing/](https://hansgaron.com/articles/web_audio/audio_tag_and_scrubbing/)  
27. Style-Bert-VITS2/docs/README\_en.md at master \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/Sunwood-ai-labs/Style-Bert-VITS2/blob/master/docs/README\_en.md](https://github.com/Sunwood-ai-labs/Style-Bert-VITS2/blob/master/docs/README_en.md)  
28. Style Bert Vits2 Pretrained Model Ver2 \- Dataloop AI, 5月 17, 2025にアクセス、 [https://dataloop.ai/library/model/ayousanz\_style-bert-vits2-pretrained-model-ver2/](https://dataloop.ai/library/model/ayousanz_style-bert-vits2-pretrained-model-ver2/)  
29. The most powerful AI audio API and detailed documentation \- ElevenLabs, 5月 17, 2025にアクセス、 [https://elevenlabs.io/developers](https://elevenlabs.io/developers)  
30. Models | ElevenLabs Documentation, 5月 17, 2025にアクセス、 [https://elevenlabs.io/docs/models](https://elevenlabs.io/docs/models)  
31. Comparison of FastAPI with Django and Flask \- GeeksforGeeks, 5月 17, 2025にアクセス、 [https://www.geeksforgeeks.org/comparison-of-fastapi-with-django-and-flask/](https://www.geeksforgeeks.org/comparison-of-fastapi-with-django-and-flask/)  
32. Which Is the Best Python Web Framework: Django, Flask, or FastAPI? | The PyCharm Blog, 5月 17, 2025にアクセス、 [https://blog.jetbrains.com/pycharm/2025/02/django-flask-fastapi/](https://blog.jetbrains.com/pycharm/2025/02/django-flask-fastapi/)  
33. WebSockets \- FastAPI, 5月 17, 2025にアクセス、 [https://fastapi.tiangolo.com/advanced/websockets/](https://fastapi.tiangolo.com/advanced/websockets/)  
34. Building LLM Applications with Hugging Face Endpoints and FastAPI \- MachineLearningMastery.com, 5月 17, 2025にアクセス、 [https://machinelearningmastery.com/building-llm-applications-with-hugging-face-endpoints-and-fastapi/](https://machinelearningmastery.com/building-llm-applications-with-hugging-face-endpoints-and-fastapi/)  
35. LangChain Tutorial in Python \- Crash Course, 5月 17, 2025にアクセス、 [https://www.python-engineer.com/posts/langchain-crash-course/](https://www.python-engineer.com/posts/langchain-crash-course/)  
36. Tutorials \- ️ LangChain, 5月 17, 2025にアクセス、 [https://python.langchain.com/docs/tutorials/](https://python.langchain.com/docs/tutorials/)  
37. I've made a production-ready Fastapi LangGraph template : r/LangChain \- Reddit, 5月 17, 2025にアクセス、 [https://www.reddit.com/r/LangChain/comments/1juejy2/ive\_made\_a\_productionready\_fastapi\_langgraph/](https://www.reddit.com/r/LangChain/comments/1juejy2/ive_made_a_productionready_fastapi_langgraph/)  
38. Step-by-step Guidelines for Integrating GPT in Your Project: Create an API for Anything Using LangChain and FastAPI \- DEV Community, 5月 17, 2025にアクセス、 [https://dev.to/afrozansenjuti/integrating-gpt-in-your-project-create-an-api-for-anything-using-langchain-and-fastapi-1h77](https://dev.to/afrozansenjuti/integrating-gpt-in-your-project-create-an-api-for-anything-using-langchain-and-fastapi-1h77)  
39. Total noob's intro to Hugging Face Transformers, 5月 17, 2025にアクセス、 [https://huggingface.co/blog/noob\_intro\_transformers](https://huggingface.co/blog/noob_intro_transformers)  
40. Hugging Face Transformers: Leverage Open-Source AI in Python, 5月 17, 2025にアクセス、 [https://realpython.com/huggingface-transformers/](https://realpython.com/huggingface-transformers/)  
41. A super lightweight API server for Style-Bert-VITS2 that can run anywhere\! \- GitHub Gist, 5月 17, 2025にアクセス、 [https://gist.github.com/uezo/1b360926209beb818362fd5688a0fd8d](https://gist.github.com/uezo/1b360926209beb818362fd5688a0fd8d)  
42. CLI.md \- litagin02/Style-Bert-VITS2 \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/litagin02/Style-Bert-VITS2/blob/master/docs/CLI.md](https://github.com/litagin02/Style-Bert-VITS2/blob/master/docs/CLI.md)  
43. litests \- PyPI, 5月 17, 2025にアクセス、 [https://pypi.org/project/litests/0.3.8/](https://pypi.org/project/litests/0.3.8/)  
44. WebSockets vs. HTTP: Real-Time Communication Deep Dive \- VideoSDK, 5月 17, 2025にアクセス、 [https://www.videosdk.live/developer-hub/websocket/websockets-vs-http](https://www.videosdk.live/developer-hub/websocket/websockets-vs-http)  
45. HTTP vs WebSocket: Real-Time Web Communication Guide \- Digital Samba, 5月 17, 2025にアクセス、 [https://www.digitalsamba.com/blog/websocket-vs-http](https://www.digitalsamba.com/blog/websocket-vs-http)  
46. kthwaite/fastapi-websocket-broadcast \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/kthwaite/fastapi-websocket-broadcast](https://github.com/kthwaite/fastapi-websocket-broadcast)  
47. Mastering FastAPI: A Comprehensive Guide to Serving Static Files Efficiently \- Mathison AG, 5月 17, 2025にアクセス、 [https://mathison.ch/en-ch/blog/mastering-fastapi-a-comprehensive-guide-to-serving/](https://mathison.ch/en-ch/blog/mastering-fastapi-a-comprehensive-guide-to-serving/)  
48. DanielSWolf/rhubarb-lip-sync \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/DanielSWolf/rhubarb-lip-sync](https://github.com/DanielSWolf/rhubarb-lip-sync)  
49. mrxz/wLipSync: MFCC-based lip-sync library using WASM and WebAudio based on uLipSync \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/mrxz/wLipSync](https://github.com/mrxz/wLipSync)  
50. despossivel \- NPM, 5月 17, 2025にアクセス、 [https://www.npmjs.com/\~despossivel](https://www.npmjs.com/~despossivel)  
51. keywords:speech-recognition \- npm search, 5月 17, 2025にアクセス、 [https://www.npmjs.com/search?q=keywords%3Aspeech-recognition\&page=1\&perPage=20](https://www.npmjs.com/search?q=keywords:speech-recognition&page=1&perPage=20)  
52. WASM Tutorial \- Marco Selvatici, 5月 17, 2025にアクセス、 [https://marcoselvatici.github.io/WASM\_tutorial/](https://marcoselvatici.github.io/WASM_tutorial/)  
53. lipSync.ts \- pixiv/ChatVRM \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/pixiv/ChatVRM/blob/main/src/features/lipSync/lipSync.ts](https://github.com/pixiv/ChatVRM/blob/main/src/features/lipSync/lipSync.ts)  
54. Blendshape Animation for Visemes slow \- Questions \- three.js forum, 5月 17, 2025にアクセス、 [https://discourse.threejs.org/t/blendshape-animation-for-visemes-slow/61681](https://discourse.threejs.org/t/blendshape-animation-for-visemes-slow/61681)  
55. Morph Targets | Babylon.js Documentation, 5月 17, 2025にアクセス、 [https://doc.babylonjs.com/features/featuresDeepDive/mesh/morphTargets](https://doc.babylonjs.com/features/featuresDeepDive/mesh/morphTargets)  
56. How to Build an Inference API using Hugging Face Diffusers and FastAPI \- Knowledgebase, 5月 17, 2025にアクセス、 [https://rcs.is/knowledgebase/836/How-to-Build-an-Inference-API-using-Hugging-Face-Diffusers-and-FastAPI.html](https://rcs.is/knowledgebase/836/How-to-Build-an-Inference-API-using-Hugging-Face-Diffusers-and-FastAPI.html)  
57. uezo/ChatdollKit: ChatdollKit enables you to make your 3D ... \- GitHub, 5月 17, 2025にアクセス、 [https://github.com/uezo/ChatdollKit](https://github.com/uezo/ChatdollKit)